# Bootstrapping the Kubernetes Control Plane

## Only Controller-0 node
## SSH to controller-0 Nodes
```
controller-0:

external_ip=$(aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=controller-0" "Name=instance-state-name,Values=running" \
  --output text --query 'Reservations[].Instances[].PublicIpAddress')
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
  -i example-k8s.id_rsa ubuntu@${external_ip}
```


## Create kubeadm config
“controlPlaneEndpoint” from External IP created in Kubernetes Public IP Address
```
cat > kubeadm-config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "18.204.156.10:6443"
networking:
    podSubnet: "10.244.0.0/16"
EOF
```
                                
## Initialize the Cluster
```
sudo kubeadm init --config=kubeadm-config.yaml --upload-certs
```

## Initialize Result
```
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/
You can now join any number of the control-plane node running the following command on each as root:
kubeadm join 18.204.156.10:6443 --token cqcine.t9org9a13qaosvfs \
        --discovery-token-ca-cert-hash sha256:aa34613fafaff18510bb41cadb08145599a5d9bac3c9848a69b576ded0fbf133 \
        --control-plane --certificate-key ef3c15ee490e1e609eada54b140d9f05d5237ff53ce78cbeda9bcc0d5f2437cf
Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.
Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 18.204.156.10:6443 --token cqcine.t9org9a13qaosvfs \
        --discovery-token-ca-cert-hash sha256:aa34613fafaff18510bb41cadb08145599a5d9bac3c9848a69b576ded0fbf133
```

                                
## Make Current user able to use kube commands
```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
                                
## Install Calico Networking (CNI)
                                
```
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
kubectl get pods -n kube-system
```

## exit
```                                
exit
```
                                
## All Master nodes except controller-0
## SSH to controller-1 and controller-2 Nodes
```
controller-1:

external_ip=$(aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=controller-1" "Name=instance-state-name,Values=running" \
  --output text --query 'Reservations[].Instances[].PublicIpAddress')
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
  -i example-k8s.id_rsa ubuntu@${external_ip}

controller-2:

external_ip=$(aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=controller-2" "Name=instance-state-name,Values=running" \
  --output text --query 'Reservations[].Instances[].PublicIpAddress')
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
  -i example-k8s.id_rsa ubuntu@${external_ip}
```
                                
## Join master nodes
```                                
sudo kubeadm join 18.204.156.10:6443 --token cqcine.t9org9a13qaosvfs \
        --discovery-token-ca-cert-hash sha256:aa34613fafaff18510bb41cadb08145599a5d9bac3c9848a69b576ded0fbf133 \
        --control-plane --certificate-key ef3c15ee490e1e609eada54b140d9f05d5237ff53ce78cbeda9bcc0d5f2437cf
```

## Make Current user able to use kube commands
```                                
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## exit
```
exit
```

## SSH to controller-0 Nodes
```
controller-0:

external_ip=$(aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=controller-0" "Name=instance-state-name,Values=running" \
  --output text --query 'Reservations[].Instances[].PublicIpAddress')
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
  -i example-k8s.id_rsa ubuntu@${external_ip}
```

## Verify All master nodes already join
```
ubuntu@ip-10-240-0-10:~$ kubectl get nodes
NAME             STATUS   ROLES                  AGE     VERSION
ip-10-240-0-10   Ready    control-plane,master   9m35s   v1.22.1
ip-10-240-0-11   Ready    control-plane,master   2m27s   v1.22.1
ip-10-240-0-12   Ready    control-plane,master   54s     v1.22.1
```

## exit
```
exit
```
                                
## Add controller-1 and controller-2 in target pool Load Balancer from aws cli
```                                
aws elbv2 register-targets \
  --target-group-arn ${TARGET_GROUP_ARN} \
  --targets Id=10.240.0.1{1,2}
```

Next: [Part 7 - Bootstrapping the Kubernetes Worker Nodes](08-part-07.md)
